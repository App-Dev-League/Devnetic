{"type":"lesson","pages":[{"type":"information","title":"Week 7 - Natural Language Processing","elements":[{"type":"text","content":"[[h3]]What is Natural Language Processing?[[/]]\n➔ Subfield of AI that focuses on understanding language\n➔ Has its own subfields in which there are different tasks\n➔ There can include text classification, next sentence prediction, and masking\n➔ We can play with many of these subfields through the website below\n➔ https://demo.allennlp.org/"},{"type":"image","src":"/data/modules/ai-assets/img6/0-0.png","width":"20%","float":"right"},{"type":"text","content":"[[h3]]Word Embeddings[[/]]\n➔ We cannot simply parse words into our models for any evaluation\n➔ Models work with numbers, so we must represent the words that way\n➔ This is done with word embeddings (word vectors)\n➔ Essentially list of numbers of n dimensions"}]},{"type":"information","title":"Week 7 - Natural Language Processing","elements":[{"type":"text","content":"[[h3]]Converting words to vectors[[/]]\n➔ How do we convert our words to embeddings\n➔ Visualized here: https://projector.tensorflow.org/ (also called word cloud or a semantic space)\n➔ Generally speaking, word vectors will be either 100, 200, or 300 dimensions\n[[h3]]One-hot encoding[[/]]\n➔ Uses a matrix of 0s, where each value is represented by a single 1 (or a “hot” value)\n➔ Inefficient; left with very large matrices of mostly 0s"},{"type":"image","src":"/data/modules/ai-assets/img6/1-0.png","width":"50%","float":"left"},{"type":"text","content":"    ◆ Value 0 is shown to be a banana, aka the value 100\n        ➔ By similar logic, values 1 and 3 are a apples (010) and value 2 is a cherry (001)"}]},{"type":"information","title":"Week 7 - Natural Language Processing","elements":[{"type":"text","content":"[[h3]]Contextual Word Embeddings[[/]]\n➔ One difference from regular word embeddings\n    ◆ Traditional embeddings = one vector for one word\n        ➔ Disadvantage: polysemy = one word means multiple things\n            ◆ Take for example the word “fly”:\n                ➔ Noun: “This fly is so annoying”\n                ➔ Adjective: “You look pretty fly today”\n                ➔ Verb: “I want to fly a plane”\n➔ Change the vector based on the context of the word → solve polysemy\n➔ Takes the part-of-speech of the word as part of the input to the word embedding\n➔ ELMo embeddings = popular software"},{"type":"image","src":"/data/modules/ai-assets/img6/2-0.png","width":"35%","float":"right"},{"type":"text","content":"[[h3]]Text Classification[[/]]\n➔ Text Classification consists of several different tasks\n➔ They all will output some classification metric about the input sentence\n➔ For example: Spam messages, Sentiment Analysis, intent classification"}]},{"type":"information","title":"Week 7 - Natural Language Processing","elements":[{"type":"text","content":"[[h3]]Sentiment Analysis[[/]]\n➔ A subfield of Text Classification that focuses primarily on deriving subjectivity from text\n    ➔ What type of emotion does this unstructured data display?\n    ➔ https://text2data.com/Demo\n➔ Can be binary or fine-grain (although binary is more common)\n➔ Polarity = whether the sentiment is positive or negative\n➔ Intensity = how positive or negative the sentiment is (fine-grain)"},{"type":"image","src":"/data/modules/ai-assets/img6/3-0.png","width":"25%","float":"right"},{"type":"text","content":"[[h3]]Machine Translation (MT)[[/]]\n➔ Translates one language to another using AI\n➔ More efficient than human translation\n➔ Most popular: Statistical (SMT), Rule-based (RBMT), Hybrid (HMT), Neural (NMT)\n    ➔ SMT: Rely on statistical models that investigate bilingual text volumes to find correspondences between words\n    ➔ RBMT: Based on grammatical/linguistic rules of each language\n    ➔ HMT: Multiple MT approaches to form one MT system (common = mix of RBMT + SMT)\n    ➔ Neural: Depend on neural network models to build statistical models that translate text"}]},{"type":"information","title":"Week 7 - Natural Language Processing","elements":[{"type":"text","content":"[[h3]]Language Modelling[[/]]\n➔ Aims to predict and then create language\n    ➔ Determine probability of sequence of words occurring in sentence\n➔ The following are examples of AI generated names"},{"type":"image","src":"/data/modules/ai-assets/img6/4-0.png","width":"25%","float":"left"},{"type":"image","src":"/data/modules/ai-assets/img6/4-1.png","width":"40%"},{"type":"text","content":"➔ The link below is a great example of the GPT-3 model trained for language generation (modelling but produces large amounts of output text)\n    ➔ It produces large swaths of relatively comprehensible text\n➔ https://bellard.org/textsynth/\n➔ Try this for masked language modelling: https://demo.allennlp.org/masked-lm\n[[h3]]Naïve Bayes Classifier[[/]]\n➔ Probabilistic machine learning model used for classifying text (primarily sentiment analysis)\n➔ Assumes that predictors are independent and have an equal effect on the outcome"}]},{"type":"information","title":"Week 7 - Natural Language Processing","elements":[{"type":"image","src":"/data/modules/ai-assets/img6/5-0.png","float":"right","width":"28%"},{"type":"text","content":"[[h3]]Neural Networks[[/]]\n➔ Feedforward Neural Networks have big drawbacks (being unidirectional) in NLP tasks\n    ➔ Context is unable to be captured\n➔ RNNs, specifically bi-LSTMs, are preferred\n➔ Used for word embeddings as well\n[[h3]]Sequence-to-Sequence Models[[/]]\n➔ Use “encoders” and “decoders” to parse inputs one token at a time "},{"type":"image","src":"/data/modules/ai-assets/img6/5-1.png","width":"100%"},{"type":"text","content":"➔ The memory output from each step in the encoder RNN continues into the decoder\n➔ Encoder vector is then decoded by decoder RNN into output results"}]},{"type":"information","title":"Week 7 - Natural Language Processing","elements":[{"type":"image","src":"/data/modules/ai-assets/img6/6-0.png","float":"right","width":"45%"},{"type":"text","content":"[[h3]]Transformers and BERT[[/]]\n➔ BERT = Bidirectional Encoder Representations from Transformers\n➔ Essentially uses a stack of encoders and decoders\n    ➔ As opposed to a single encoder decoder\n➔ Applies bidirectional training of transformers → language modelling\n➔ Uses Masked Language Model (MLM) + Next Sentence Prediction (NSP) approach\n    ➔ MLM: 15% of words are replaced with [MASK] token → model tries to predict the [MASK] token\n    ➔ NSP: given sentences A and B, does B follow A? 50% of inputs = subsequent sentence is in the text"}]},{"type":"multiple_choice","question":"What is Natural Language Processing?","descriptions":["Correct! NLP is a subfield of AI that focuses on understanding language","NLP is not the biological process by which humans understand language","NLP has nothing to do with animal languge","NLP is a subfield of AI, not the academic field including all of AI"],"points":5,"coins":1,"answers":["Subfield of AI that focuses on understanding language","The biological process by which humans understand language","The study of animal language-related behavior","The academic field that encompasses all of AI"],"correct":0},{"type":"multiple_choice","question":"Which of the following are NLP tasks?","descriptions":["Linear regression is not a Natural Language Processing task","Image Classification is not a Natural Language Processing task","Correct! Text classification is an NLP task that includes sentiment analysis and more","Audio recording is not an NLP task"],"points":5,"coins":1,"answers":["Linear Regression","Image Classification","Text Classification","Audio Recording"],"correct":2},{"type":"short_answer","question":"What is the dimensionality of the following vector: [3, 7, 6, 3, 10, 8, 2, 5, 4, 2]?","code_template":"","answers":["10","ten","Ten"],"descriptions":{},"description_default":"The dimensionality of a vector is defined as its length","points":10,"coins":3},{"type":"multiple_choice","question":"What is Word Embedding?","descriptions":["Correct! Word embedding is used to turn text into numbers that our statistical AI models can process","Word Embedding does not aim to find the contextual meaning of a word. Think about the fact that we need to use word embeddings for our models rather than words.","Word Embedding does not parse test from input streams. Think about the fact that we need to use word embeddings for our models rather than words.","Word embedding does not create new words. Think about the fact that we need to use word embeddings for our models rather than words."],"points":5,"coins":1,"answers":["A way to turn text into numerical data for model processing","A way to find the meaning of a word by embedding it in a sentence","A way to parse text from an input stream","A method for creating new words by embedding other ones"],"correct":0},{"type":"multiple_choice","question":"How many dimensions is a typical word?","descriptions":["Generally speaking, 1-10 dimensions are too small to hold a word.","Correct! Most words are between 100-300 dimensions","Generally speaking, 10-30 dimensions are too small to hold a word.","Generally speaking, most words will not reach 1000-3000 dimensions."],"points":5,"coins":1,"answers":["1 - 10","100 - 300","10 - 30","1000 - 3000"],"correct":1},{"type":"multiple_choice","question":"How do we vectorize sentences?","descriptions":["We cannot use embedding for an entire sentence at once. Embeddings work only for words.","We cannot use embedding for an entire sentence at once. Embeddings work only for words.","Using only the first word in the sentence would mean that many parts of the sentence are excluded.","Correct! By converting the sentence into an array of words, we can use word embeddings on all of them and then flatten it back into a sentence."],"points":5,"coins":1,"answers":["Use word embedding for the entire sentence at once, including spaces","Use word embedding for the entire sentence at once, excluding spaces","Use only the first word in the sentence","Convert the sentence into an array and use word embedding and flatten"],"correct":3},{"type":"multiple_choice","question":"Why is one-hot encoding inefficient?","descriptions":["One hot encoding doesn't have anything to do with physical temperature.","One-hot encoding can be slow but this is not the main reason that its inefficient.","Correct! One-hot encoding uses extra space by leaving us with a largely unfilled matrix that is mostly 0s.","One-hot encoding is not particularly difficult to code."],"points":5,"coins":1,"answers":["It's too hot","It takes too much time","We are left with a large matrix of mostly 0s","It is very difficult to code"],"correct":2},{"type":"multiple_choice","question":"What is one problem with traditional word embeddings?","descriptions":["Correct! Traditional word embeddings simply vectorize a word without taking into account its context","Traditional word embeddings do not take much more space than any other embedding or vectorization process","Traditional word embeddings do not take much more time than any other embedding or vectorization process","Different spellings are not necessarily part of the embedding process and can usually be considered as separate words"],"points":5,"coins":1,"answers":["It does not consider the context (multiple meanings)","It takes up tons of space","It is slow","It does not account for different spellings of the same word"],"correct":0},{"type":"short_answer","question":"What is the term for the phenomenon where a word has multiple meanings?","code_template":"","answers":["polysemy","Polysemy"],"descriptions":{},"description_default":"The dimensionality of a vector is defined as its length","points":10,"coins":3},{"type":"multiple_choice","question":"What additional input do contextual word embeddings need that is different from traditional word embeddings?","descriptions":["Correct! Contextual word embeddings use the part of speech to figure out the contextual meaning of the word in the sentece","Contextual word embeddings do not need the position of the word in the sentence as an input","Contextual word embeddings do not need the meaning of the word in the sentence as an input","Contextual word embeddings do not need the length of the word as an input"],"points":5,"coins":1,"answers":["Part of Speech","Position in sentence","Meaning in sentence","Length of the word"],"correct":0},{"type":"multiple_choice","question":"Sentiment analysis is what type of NLP task?","descriptions":["Sentiment analysis is not a language modelling task. Think about the fact that it classifies text based on the sentiment.","Sentiment analysis is not a spam filtration task. In fact, spam filtration is not a category of NLP tasks but rather a specific task in itself. Think about the fact that it classifies text based on the sentiment.","Correct! Sentiment analysis is a text classification task that classifies texts based on their sentiment.","Sentiment analysis is not a machine translation task. Think about the fact that it classifies text based on the sentiment."],"points":5,"coins":1,"answers":["Lanugage Modelling","Spam filtration","Text classification","Machine Translation"],"correct":2},{"type":"multiple_choice","question":"Which of the following is NOT a text classification task","descriptions":["Correct! Machine translation is not a text classification task","Intent Classification is a text classification task","Spam filtration is a text classification task","Sentiment Analysis is a text classification task"],"points":5,"coins":1,"answers":["Machine Translation","Intent Classification","Spam Filtration","Sentiment Analysis"],"correct":0},{"type":"multiple_choice","question":"What are the two types of sentiment analysis?","descriptions":["Correct! Although binary is more common, both binary and fine-grain are types of sentiment analysis","Multiple is not a type of sentiment analysis","Multiple is not a type of sentiment analysis","Polarity and intensity are two parts of sentiment analysis but polar and intense are not the two types of sentiment analysis"],"points":5,"coins":1,"answers":["Binary and Fine-Grain","Binary and Multiple","Multiple and Fine-Grain","Polar and intense"],"correct":0},{"type":"short_answer","question":"What is the term for a measure of whether a sentiment is positive or negative?","code_template":"","answers":["polarity","Polarity"],"descriptions":{"Intensity":"Intensity is a measure of HOW positive or negative a sentiment is"},"description_default":"Think of positive and negative as two opposite poles","points":10,"coins":3},{"type":"multiple_choice","question":"Which of the following is NOT a reason for which we might use Machine Translation over human translation?","descriptions":["Machine Translation is more efficient than human translation","Machine Translation typically costs less than human translation","Correct! Machine Translation is not more accurate than human translation","Machine Translation is much faster than human translation"],"points":5,"coins":1,"answers":["It is more efficient","It costs less to use","It is more accurate","It is faster"],"correct":2},{"type":"multiple_choice","question":"Which of the following is not a type of machine translation?","descriptions":["Correct! Artificial is not a type of machine translation","Statistical Machine translation relies on statistical models that investigate bilingual text volumes to find correspondences between words","Rule-based Machine Translation is based on grammatical/linguistic rules of each language","Hybrid Machine Translation uses multiple MT approaches to form one MT system (common = mix of RBMT + SMT)"],"points":5,"coins":1,"answers":["Artificial","Statistical","Rule-based","Hybrid"],"correct":0},{"type":"multiple_choice","question":"Which of the following best describes language modelling?","descriptions":["Correct! Language modelling aims to predict and then create language","Language modelling does not solely come up with names","Language modelling does not classify text","Language modelling does not classify text"],"points":5,"coins":1,"answers":["It aims to predict and then create language","It aims to come up with names only","It aims to classify text based on some parameter","It aims to classify text based on the language"],"correct":0},{"type":"multiple_choice","question":"What is the Naïve Bayes Classifier?","descriptions":["The Naïve Bayes Classifier is not a word embedding method. Think about the word classifier","Correct! The Naïve Bayes Classifier is a machine learning model used to classify text (usually for sentiment analysis)","While the Naïve Bayes Classifier is primarily used for sentiment analysis, it is by no means limited to just this task","The Naïve Bayes Classifier does not convert sentences to vectors. Think about the word classifier"],"points":5,"coins":1,"answers":["A word embedding method","Probabilistic machine learning model used for classifying text","A classifier limited to only sentiment analysis","A method to convert sentences to vectors"],"correct":1},{"type":"short_answer","question":"The Naïve Bayes classifier assumes that predictors are ______ and have an equal effect on the outcome","code_template":"","answers":["independent","Independent"],"descriptions":{},"description_default":"If the predictors do not affect each other and have an equal affect on the outcome, what could they be assumed to be?","points":10,"coins":3},{"type":"multiple_choice","question":"What type of neural network is preferred for NLP?","descriptions":["Correct! RNNs, specifically bi-LSTMs, are preferred for NLP tasks","CNNs are used primarily for image classification tasks (see week 6)","TNNs are a variation over Convolutional Neural Networks for sequence modelling tasks, used primarily for image-based tasks","Artificial Neural Networks have big drawbacks (being unidirectional) in NLP tasks as they can't capture context"],"points":5,"coins":1,"answers":["RNN (Recurrent Neural Network)","CNN (Convolutional Neural Network)","TNN (Temporal Neural Network)","ANN (Artificial Neural Network)"],"correct":0},{"type":"multiple_choice","question":"What are sequence-to-sequence models?","descriptions":["Sequence-to-sequence models use a series of encoders and decoders rather than multiple encoders and one decoder","Sequence-to-sequence models do not break cryptographic encoding","Correct! Sequence-to-sequence models use a series of encoders and decoders coupled with RNNs to process inputs one token at a time","Sequence-to-sequence models are not the name given to models who work with different types of sequences"],"points":5,"coins":1,"answers":["Models that use a sequence of encoders and a decoder at the end","Models that use decoders to break cryptographic encoding","Models that use a sequence of encoders and decoders with RNNs ","Models that work with many different types of sequences "],"correct":2},{"type":"multiple_choice","question":"What NLP task does BERT perform?","descriptions":["Correct! BERT performs language modelling through the bidirectional training of transformers","BERT does not directly perform text classification","BERT does not directly perform machine translation","BERT does not perform tokenization"],"points":5,"coins":1,"answers":["Language Modelling","Text Classification","Machine Translation","Tokenization"],"correct":0},{"type":"congratulations","title":"Congratulations!","elements":[{"type":"text","content":"In this module, you learned all about Natural Language Processing:\n[[h3]]Concept of NLP[[/]]\n➔ Subset of AI that focuses on understanding and modelling language\n➔ Consists of several subfields that cover more specific tasks\n➔ Use convolutions (filters) to work\n[[h3]]Word Embeddings[[/]]\n➔ A method or structure that is employed to convert words into numbers (or in this case vectors)\n➔ Each word in the dictionary is represented by a vector in a n-dimensional space (referred to in NLP as a semantic space)\n➔ Similar words will have similar vectors (and therefore lower cosine distance from each other in the semantic space)\n➔ Most word vectors offered today will be either 100, 200, or 300 dimensions\n➔ Why are word vectors useful?\n    ➔ Most models need to deal directly with numbers\n    ➔ A neural network, for example, will not be able to take words as an input, and thus meaningfully representing them numerically is important\n[[h3]]One-hot encoding[[/]]\n➔ Uses a matrix of 0s, where each value is represented by a single 1 (or a “hot” value)\n➔ Unfortunately one-hot is rather inefficient, as one is left with a very large matrices of mostly 0s\n[[h3]]Contextual Word Embeddings[[/]]\n➔ Essentially same as regular word embeddings, with one special change\n    ➔ Traditional embeddings will have only one vector for one word\n        ➔ Though this poses a problem due to polysemy (where one word means multiple things)\n➔ To solve polysemy, we can change the vector based on the context of the word\n➔ This works by taking the PoS (part-of-speech) of the word as part of the input to the word embedding\n[[h3]]Text Classification[[/]]\n➔ Subtask of NLP: Tries to organize text input into some predefined categories\n➔ Includes sentiment analysis, spam detection, and intent classification\n[[h3]]Sentiment Analysis[[/]]\n➔ Subtask of NLP; also example of Text Classification\n➔ Tries to determine “sentiment” or subjectivity from text\n➔ Polarity = sentiment is positive? negative?\n➔ Intensity = how positive/negative is the sentiment?\n[[h3]]Machine Translation[[/]]\n➔ Translates one language to another\n➔ SMT, RBMT, HMT, Neural models for machine translation\n[[h3]]Language Modelling[[/]]\n➔ Can create language by first predicting it\n➔ Useful for finishing sentences\n➔ [[a]]https://bellard.org/textsynth [[/a]]\n➔ [[link=https://demo.allennlp.org/masked-lm]]Masked language modelling[[/link]]\n[[h3]]Naïve Bayes Classifier[[/]]\n➔ Probabilistic machine learning model used for classifying text\n➔ Naïve because assumes predictors are independent\n[[h3]]Neural Networks[[/]]\n➔ Used for word embeddings\n➔ Feedforward networks = not preferred for NLP because they are not bidirectional\n➔ RNNs (biLSTMS) = more helpful\n    ➔ Bidirectionality is important in understanding text\n[[h3]]Sequence-to-sequence Models[[/]]\n➔ Encoders + decoders for input sequences\n➔ Encoder vector → decoder → output\n[[h3]]Transformers and BERT[[/]]\n➔ Difference from seq2seq: stack of encoders + decoders\n➔ Bidirectional training\n➔ MLM + NSP approach"}]}]}