{"type":"lesson","pages":[{"type":"information","title":"Week 4 - Decision Trees","elements":[{"type":"text","content":"[[h3]]Decision Trees[[/]]\n➔ Consider this scenario: you are a doctor and a person walks in to your hospital with a lot of different symptoms. Your job is to diagnose what disease they have. What is the most efficient way to do this?\n➔ One way to do this would be to make a tree considering potential combinations"},{"type":"image","src":"./data/modules/ai-assets/img3/0-3.png","width":"100%"},{"type":"text","content":"[[h3]]What is a Decision Tree?[[/]]\n➔ A branching diagram that represents choices to get to a final result\n➔ Usually, we try to solve some problem in ML with a huge tree\n➔ Can be used for regression and classification(clustering) tasks\n[[h3]]Why use them?[[/]]\n➔ Simple to understand, you can easily view them as a branching tree\n➔ Can handle many kinds of data including numerical and categorical data\n➔ Forces an outcome to be formed, never returns a null prediction\n[[h3]]Some reasons NOT to use them[[/]]\n➔ Data is often overfit and can cause issues especially if the training data has noise\n➔ Low bias makes the tree hard to use with new data\n➔ Training time is often larger, which may be a big problem for costs and feasibility on larger datasets"}]},{"type":"information","title":"Week 4 - Random Forest","elements":[{"type":"text","content":"[[h3]]Random Forest[[/]]\n➔ Used to solve both regression and classification tasks\n➔ Uses many decision trees (hence forest) and takes their average to make predictions\n➔ The more the trees, the more accurate Random Forest is\n➔ Eradicates limitations of decision trees, prevents overfitting and increases precision\n[[h3]]Methods[[/]]\n➔ Instead of determining where the tree splits, Random Forest uses randomness to generate each order of nodes where the tree splits\n➔ Example: To decide whether to buy something, 4 factors affect the buyers decision\n    ➔ RF will generate many trees with random permutations of those 4 factors as different trees, the highest voted decision (buy or don’t buy) will be picked"}]},{"type":"information","title":"Week 4 - XGBoost","elements":[{"type":"text","content":"[[h3]]XGBoost[[/]]\n➔ “Extreme Gradient Boost” helps improve and speed up decision trees through a process called gradient boosting.\n➔ Hugely popular in modern machine learning tasks, this has been dominating competitions where tabulated/structured data is present.\n[[h3]]Methods[[/]]\n➔ The inner workings of XGBoost are based on the gradient boost algorithm\n➔ Gradient boost uses the average of the target “y” values to make the first prediction, and then makes trees based on errors in that prediction by calculating an error gradient\n➔ Over time, each tree gets better at predicting the result"}]},{"type":"information","title":"Week 4 - Summary","elements":[{"type":"text","content":"[[h3]]Summary[[/]]\n➔ “By using Decision Trees, we can do a variety of machine learning tasks through many different methods\n➔ While decision trees by themselves are often accurate, Random Forest and Extreme Gradient Boost often outperform them\n➔ The parameters in a project matter a lot, try changing them up and searching up how you can make this more effective!"}]}]}