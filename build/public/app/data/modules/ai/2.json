{"type":"lesson","pages":[{"type":"information","title":"Week 3 - Regression and SVMs - Context","elements":[{"type":"text","content":"[[h3]]Regression Context[[/]]\n➔ The earliest forms of regression were published by Legendre in 1805, and by Gauss in 1809\n➔ Before coming to the world of AI and ML, regression was purely mathematical in nature\n➔ In recent years, regression has become more robust and has entered many fields that involve some kind of forecasting\n➔ The types we will be covering are linear regression and polynomial regression"},{"type":"image","src":"./data/modules/ai-assets/img2/0-2.png","width":"50%"}]},{"type":"information","title":"Week 3 - Regression and SVMs - Types","elements":[{"type":"text","content":"[[h3]]Types of Regression[[/]]\n➔ Two basic forms: simple linear regression, multiple linear regression\n➔ Simple Linear Regression: y = mx + b, where there is only one outcome variable y defined by a single input variable x\n➔ Multiple Linear Regression: y = b + m1x1 + m2x2 + m3x3… + mnxn, where n is the number of input variables\n➔ Simple linear regression: line\n➔ Multiple linear regression: n-dimensional plane"},{"type":"image","src":"./data/modules/ai-assets/img2/1-2.png","width":"50%"},{"type":"image","src":"./data/modules/ai-assets/img2/2-2.png","width":"50%"}]},{"type":"information","title":"Week 3 - Regression and SVMs - Methods","elements":[{"type":"text","content":"[[h3]]Method[[/]]\n➔ Pick an arbitrary line/plane to fit the data, sometimes using random and sometimes using averages\n➔ Measure the error and adjust the line/plane to minimize it using error calculation methods\n➔ Repeat until the best line is found\n➔ Once the best line is found, we can make predictions for future values\n[[h3]]Error[[/]]\n➔ The difference between known y-value and what our model gives, calculated to improve the model\n➔ What is the best way to measure error?\n    ➔ Simply summing the differences between the predicted vs actual values is okay but not great\n    ➔ Ordinary Least Squares: Take the sum of the squares of the differences as a measure for error; has shown to be better in practice\n    ➔ Many other methods, but we will focus on OLS"},{"type":"image","src":"./data/modules/ai-assets/img2/3-2.png","width":"50%"}]},{"type":"information","title":"Week 3 - Regression and SVMs - Machines","elements":[{"type":"text","content":"[[h3]]Support Vector Machines[[/]]\n[[h3]]Context[[/]]\n➔ Developed at Bell Labs in 1992\n➔ Based on statistical frameworks from Vapnik and Chervonenkis \n➔ One of the most robust predictions methods\n➔ Given a set of data, SVM finds a line that splits the data in two categories, such as to maximize the separation between the two categories"},{"type":"image","src":"./data/modules/ai-assets/img2/4-2.png","width":"50%"},{"type":"text","content":"[[h3]]Method[[/]]\n➔ The hard margin is the sum of the distances between the right-most red and leftmost green to the line that cuts the data in half. We want to maximize this"},{"type":"image","src":"./data/modules/ai-assets/img2/5-2.png","width":"50%"},{"type":"text","content":"[[h3]]Motivation[[/]]\n➔ Why should we be doing any of this?\n➔ Classification is difficult for computers, especially with images\n➔ If we can plot features on a n-dimensional graph and find the best line/plane of separation, we can form an easy way to predict future values on this graph"}]}]}