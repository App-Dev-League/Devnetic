{
    "type": "lesson",
    "pages": [
        {
            "type": "information",
            "title": "Week 8 - Unsupervised Learning",
            "elements": [
                {
                    "type": "text",
                    "content": "[[h3]]Unsupervised Learning[[/]]\n➔ The transition from supervised to unsupervised learning can be complex, so we will try to break it down here\n➔ Unlike supervised learning, where data is labeled and we are trying to maximize a certain outcome, unsupervised learning is all about hidden patterns\n➔ We try to discover unknown patterns in data to find anomalies and other factors that can help us model data."
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/0-0.png",
                    "width": "57%",
                    "float": "left"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/0-1.png",
                    "width": "38%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": "[[h3]]Methods of unsupervised learning[[/]]\n➔ Clustering\n    ➔ K-means, mixture models, DBSCAN, and OPTICS algorithm\n➔ Anomaly Detection\n    ➔ Local Outlier Factor and Isolation Forest\n➔ Neural Networks\n    ➔ Autoencoders, deep belief networks, Hebbian learning, generative adversarial networks (GANs), and self-organizing maps"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/0-2.png",
                    "width": "59%",
                    "float": "left"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/0-3.png",
                    "width": "30%",
                    "float": "right"
                }
            ]
        },
        {
            "type": "information",
            "title": "Week 8 - Unsupervised Learning",
            "elements": [
                {
                    "type": "text",
                    "content": "[[h3]]K-Means Clustering[[/]]\n➔ The algorithm must self-discover clusters that some given data can be separated into\n➔ Applications range from recommendation engines to image segmentation and more\n➔ There are many ways to do K-means clustering, but typically, we start by forming k random clusters and reduce some metric from there, typically average distances between points inside a cluster"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/1-0.png",
                    "width": "40%",
                    "float": "left"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/1-1.png",
                    "width": "50%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": "[[h3]]K-means clustering Steps[[/]]"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/1-2.png",
                    "width": "10%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": "1. Choose the number of clusters, k.\n2. Choose k random points in the data to be centers"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/1-3.png",
                    "width": "10%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": "3. Assign all points to the closest center\n4. Recompute the centers of the newly formed clusters"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/1-4.png",
                    "width": "10%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": "5. Repeat steps 3 and 4 until\n    a. The centers do not change"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/1-5.png",
                    "width": "10%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": "    b. Points remain in the same cluster from iteration to iteration\n    c. It reaches a set number of maximum iterations (to save computational time and power)"
                }
            ]
        },
        {
            "type": "information",
            "title": "Week 8 - Unsupervised Learning",
            "elements": [
                {
                    "type": "text",
                    "content": "[[h3]]Implementing K-means clustering[[/]]\n➔ Usually, we will use a library that handles the complicated things for us\n➔ For now, let’s discuss how we would go about implementing this without a library\n➔ Assign a variable to hold “k,” the number of groups\n➔ Take k samples of the dataset and store them in an array called centers or centroids\n➔ Use a loop to calculate the distance from each point to each center and assign each point to a cluster of the closest center\n➔ Recompute the three centers"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/2-0.png",
                    "width": "25%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": "    ➔ For each point in a cluster, calculate its average distance to all the other points in the cluster\n    ➔ The point with the lowest average is the new center for that cluster\n➔ Recompute which cluster each point belongs to\n➔ Put all of this in a loop and continue it until:\n    ➔ The centers do not change\n    ➔ Points remain in the same cluster from iteration to iteration\n    ➔ It reaches a set number of maximum iterations"
                }
            ]
        },
        {
            "type": "information",
            "title": "Week 8 - Unsupervised Learning",
            "elements": [
                {
                    "type": "text",
                    "content": "[[h3]]Local Outlier Factor[[/]]"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/3-0.png",
                    "width": "10%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": "➔ Uses the concept of “local density” to figure out which data point is an outlier\n➔ Calculate k-nearest distances from a point to its neighbors\n➔ Compare it to the average distance of other points\n➔ If the local density of one point is too low, it is considered to be an outlier\n[[h3]]The Advantage[[/]]\n➔ Able to identify outliers in a data set that would not be outliers in another area of the data set. \n➔ For example, a point at a \"small\" distance to a very dense cluster could be an outlier, while a point within a sparse cluster might exhibit similar distances to its neighbors, meaning it is not an outlier.\n➔ Helps determine true anomalies in the data"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/3-1.png",
                    "width": "40%",
                    "float": "left"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/3-2.png",
                    "width": "53%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": "[[h3]]Subroutines/Parts[[/]]\n➔ K-distance\n    ➔ Distance between a point and its K-th nearest neighbor\n➔ Reachability Distance between 2 points\n    ➔ K minus the K-distance of the second point or the distance between two points\n    ➔ Whichever is larger"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/3-3.png",
                    "width": "40%",
                    "float": "left"
                },
                {
                    "type": "text",
                    "content": "➔ Local Reachability Distance (LRD)\n    ➔ One over the average reachability distance between a point and its neighbors\n    ➔ The farther the points, the lower the density\n➔ Local Outlier Factor (LOF)\n    ➔ The ratio of the average LRD of the K neighbors of A to the LRD of A.\n    ➔ Generally, LOF > 1 is considered to be an outlier, but it depends on the LOFs of the neighbors"
                }
            ]
        },
        {
            "type": "information",
            "title": "Week 8 - Unsupervised Learning",
            "elements": [
                {
                    "type": "text",
                    "content": "[[h3]]Autoencoders[[/]]\n➔ A type of artificial neural network used to learn efficient codings of unlabeled data\n➔ We used this in the form of the sklearn Labelencoder in previous chapters\n➔ Essentially, this converts or compresses data into some usable form by ignoring noise"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/4-0.png",
                    "width": "40%",
                    "float": "left"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/4-1.png",
                    "width": "48%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": "[[h3]]Applications[[/]]\n ➔ Dimensionality Reduction\n    ➔ The transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data\n➔ Information Retrieval\n    ➔ The process of obtaining information system resources that are relevant to an information need from a collection of those resources.\n➔ Anomaly Detection\n    ➔ The process of detecting outliers and anomalies in a dataset"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/4-2.png",
                    "width": "40%",
                    "float": "left"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/4-3.png",
                    "width": "40%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": " ➔ Drug Discovery\n    ➔ In 2019 molecules generated with variational autoencoders were validated experimentally in mice.\n➔ Popularity Prediction\n    ➔ Recently, a stacked autoencoder framework produced promising results in predicting popularity of social media posts.\n➔ Machine Translation\n    ➔ Translating text using computers"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/4-4.png",
                    "width": "40%",
                    "float": "left"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/4-5.png",
                    "width": "40%",
                    "float": "right"
                },
                {
                    "type": "text",
                    "content": " ➔ Customer Segmentation \n    ➔ Helps businesses target advertisement campaigns to consumers who would be interested, maximizing their efficiency.\n➔ Document Classification\n    ➔ Cluster documents in multiple categories based on tags, topics, and content of the document. \n➔ Image-to-image translation\n    ➔ Change a scene from day to night, or convert a sketch to what the actual object would look like"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/4-6.png",
                    "width": "40%",
                    "float": "left"
                },
                {
                    "type": "image",
                    "src": "./data/modules/ai-assets/img7/4-7.png",
                    "width": "40%",
                    "float": "right"
                }
            ]
        },
        {
            "type": "multiple_choice",
            "question": "The data we used for unsupervised learning is...",
            "descriptions": [
                "The data used for unsupervised learning may or may not be varied",
                "The data used for unsupervised learning is not too large for supervised learning",
                "Correct! The data used for supervised learning is unlabelled and the model tries to figure out patterns in this unlabelled data",
                "The data used for unsupervised learning is not too small for supervised learning"
            ],
            "points": 5,
            "coins": 1,
            "answers": [
                "Varied",
                "Too large for supervised learning",
                "Unlabelled",
                "Too small for supervised learning"
            ],
            "correct": 2
        },
        {
            "type": "multiple_choice",
            "question": "What is the goal behind unsupervised learning?",
            "descriptions": [
                "Correct! Unsupervised learning discovers patterns in data",
                "While clustering may be an unsupervised learning task, it is certainly not the only one",
                "maximizing correct predictions is the goal of some supervised learning methods",
                "Unsupervised learning is not meant to make supervised learning more efficient"
            ],
            "points": 5,
            "coins": 1,
            "answers": [
                "To discover patterns in data",
                "Only to cluster data",
                "To maximize correct predicions",
                "To supervised learning more efficient"
            ],
            "correct": 0
        },
        {
            "type": "multiple_choice",
            "question": "Which one of these is not an unsupervised learning method?",
            "descriptions": [
                "Clustering is an unsupervised learning method",
                "Anomaly detection is an unsupervised learning method",
                "Neural Netowrks can be used for unsupervised learning",
                "Correct! Regression is a supervised learning method"
            ],
            "points": 5,
            "coins": 1,
            "answers": [
                "Clustering",
                "Anomaly Detection",
                "Neural Networks",
                "Regression"
            ],
            "correct": 3
        },
        {
            "type": "short_answer",
            "question": "Local Outlier Factor and Isolation Forest are examples of what unsupervised learning method?",
            "code_template": "",
            "answers": [
                "Anomaly detection",
                "Anomaly Detection",
                "anomaly detection"
            ],
            "descriptions": {},
            "description_default": "Remember that these two algorithms detect anomalies in the data",
            "points": 10,
            "coins": 3
        },
        {
            "type": "short_answer",
            "question": "Autoencoders are examples of what unsupervised learning method?",
            "code_template": "",
            "answers": [
                "Neural Networks",
                "neural networks",
                "Neural networks",
                "neural network",
                "Neural network",
                "Neural network"
            ],
            "descriptions": {},
            "description_default": "Autoencoders are systems with weights, biases, inputs, layers, and outputs",
            "points": 10,
            "coins": 3
        },
        {
            "type": "multiple_choice",
            "question": "What does K-means clustering do?",
            "descriptions": [
                "Remember that the k is predetermined in k means clustering",
                "K means clustering does not take a clustered dataset as an input",
                "Correct! K-means clustering aims to find the best way to separate data into k clusters",
                "The metric for creating clusters is not to maximize the distance at k"
            ],
            "points": 5,
            "coins": 1,
            "answers": [
                "Come up with a number k such that clusters are most similar",
                "Find the average size of a cluster given a clustered dataset",
                "Self discover k clusters that the data can be separated into",
                "Cluster data points with a maximum distance of k between any 2 points"
            ],
            "correct": 2
        },
        {
            "type": "multiple_choice",
            "question": "Which of these is not a k-means clustering step?",
            "descriptions": [
                "We must choose k random points as our first step in K-means clustering",
                "Correct! The value of k does not change in K-means clustering",
                "We must assign all points to the closest center to measure the centers' effectiveness",
                "We must readjust clusters as needed until the right clusters are found"
            ],
            "points": 5,
            "coins": 1,
            "answers": [
                "Choose k random points in the data to be centers ",
                "Change the value of k every time a new cluster is created",
                "Assign all points to the closest center ",
                "Readjust the clusters"
            ],
            "correct": 1
        },
        {
            "type": "short_answer",
            "question": "\"K minus the K-distance of the second point or the distance between two points\" is the formula for calculating what?",
            "code_template": "",
            "answers": [
                "Reachability Distance",
                "Reachability distance",
                "reachability distance",
                "Reachability distance between two points",
                "Reachability distance between 2 points",
                "reachability distance between two points",
                "reachability distance between 2 points",
                "Reachability Distance between two points",
                "Reachability Distance between 2 points"
            ],
            "descriptions": {},
            "description_default": "This is the second subroutine of Local Outlier Factor after K-distance",
            "points": 15,
            "coins": 5
        },
        {
            "type": "multiple_choice",
            "question": "Local outlier factor is designed for which of these Unsupervised Learning subtasks?",
            "descriptions": [
                "Correct! Local outlier factor discovers outliers, AKA anomalies in a dataset",
                "LOF is not a clustering task, it finds outliers in a dataset",
                "LOF is not a neural network processing task, it finds outliers in a dataset",
                "LOF is not a pattern detection task, it finds outliers in a dataset"
            ],
            "points": 5,
            "coins": 1,
            "answers": [
                "Anomaly Detection",
                "Clustering",
                "Nerual Network Processing",
                "Pattern detection"
            ],
            "correct": 0
        },
        {
            "type": "multiple_choice",
            "question": "Which of these is the main metric for the Local Outlier Factor?",
            "descriptions": [
                "Global density would mean the density of the whole dataset and this does not give us much useful information about the point",
                "LOF does not use distance to a centroid as a main metric",
                "Correct! LOF uses the local density of a point to determine whether or not a point is an outlier",
                "LOF does not use IQR to determine whether or not a point is an outlier"
            ],
            "points": 5,
            "coins": 1,
            "answers": [
                "Global Density",
                "Distance to a centroid",
                "Local Density",
                "Interquartile Range"
            ],
            "correct": 2
        },
        {
            "type": "short_answer",
            "question": "Dimensionality _________ is an application of Autoencoders",
            "code_template": "",
            "answers": [
                "reduction",
                "Reduction"
            ],
            "descriptions": {},
            "description_default": "Transforming high-deminsional spaces into low-dimensional representations can be very helpful.",
            "points": 10,
            "coins": 3
        },
        {
            "type": "multiple_choice",
            "question": "What is an autoencoder?",
            "descriptions": [
                "Correct! An autoencoder is a type of Artificial neural network that is used to learn efficient codings of data",
                "Auteoncoders do not create ciphers to encode and decode data",
                "Autoencoders are not used to convert strings to integers",
                "Autoencoders are not meant to convert other data types to be used for processing"
            ],
            "points": 5,
            "coins": 1,
            "answers": [
                "A type of artificial neural network used to learn efficient codings of data",
                "An automatic encoder that creates a cipher to encode and decode data",
                "A method to convert strings to integers",
                "A simple encoding to handle types of data other than numbers"
            ],
            "correct": 0
        },
        {
            "type": "short_answer",
            "question": "Dimensionality _________ is an application of Autoencoders",
            "code_template": "",
            "answers": [
                "reduction",
                "Reduction"
            ],
            "descriptions": {},
            "description_default": "Transforming high-deminsional spaces into low-dimensional representations can be very helpful.",
            "points": 10,
            "coins": 3
        },
        {
            "type": "short_answer",
            "question": "What is the best student-led non-profit of them all?",
            "code_template": "",
            "answers": [
                "App Dev League",
                "ADL"
            ],
            "descriptions": {},
            "description_default": ".",
            "points": 10,
            "coins": 3
        },
        {
            "type": "congratulations",
            "title": "Congratulations!",
            "elements": [
                {
                    "type": "text",
                    "content": "In this module, you learned all about Unsupervised Learning:\n[[h3]]What is Unsupervised Learning?[[/]]\n➔ Involves recognizing hidden patterns in the dataset in order to better model data\n[[h3]]Methods of Unsupervised Learning[[/]]\n➔ Clustering\n    ➔ K-means, mixture models, OPTICS algorithm\n➔ Anomaly Detection\n    ➔ Local Outlier Factor, Isolation Forest\n➔ Neural Networks\n    ➔ Autoencoders, GANs, deep belief networks\n[[h3]]K-Means Clustering[[/]]\n➔ Algorithm self discovers clusters that data can be separated into\n➔ Start by forming K random clusters then reduce inter-cluster metric (like the average distance between points)\n[[h3]]K-Means Procedure[[/]]\n➔ Choose number of clusters, K\n➔ Choose K random points in the datasets be the centers\n➔ Assign all points to the closest center\n➔ Recompute the centers of the new clusters\n➔ Repeat steps C&D until:\n    ➔ Centers do not change\n    ➔ After each iteration, the points remain in the same cluster\n    ➔ Reach a maximum, predefined number of interactions (in the interest of time and computational power) \n[[h3]]Implementing K-Means Clustering (without an auxiliary library)[[/]]\n➔ Variable K holds the number of groups\n➔ Take K samples of the dataset, store them in an array\n➔ Loop over dataset, calculating the distance from each point to the each center, then assigning the point to its nearest center\n➔ Recompute the three centers by:\n    ➔ Calculating each point’s average distance to all the other points in its cluster using the pythagorean theorem\n    ➔ Assigning the the point with the lowest average as the new center for that cluster\n➔ Recompute which cluster each point belongs to \n➔ Repeat (using a looping construct) until: \n    ➔ The centers do not change\n    ➔ Points remain in the same cluster from iteration to iteration\n    ➔ It reaches a set number of maximum iterations\n[[h3]]Local Outlier Factor[[/]]\n➔ Figure out which data point(s) is/are outliers using “local density”\n➔ Calculate the point’s k-nearest distances to its neighbors\n➔ Compare it to the average distances of other points\n➔ Point considered an outlier if its local density is to low\n[[h3]]Advantages of Local Outlier Factor[[/]]\n➔ Better identification of Outliers\n➔ Helpful when identifying true anomalies in the dataset\n[[h3]]Subroutines/ Parts[[/]]\n➔ K Distance\n    ➔ Distance between a point and its K-th neighbor\n➔ Reachability Distance between 2 points\n    ➔ Maximum of the distance between two points and the difference between K and K Distance of a second point\n➔ Local Reachability Distance (LRD)\n    ➔ Inverse relationship representing the average reachability between a points and its neighbors\n    ➔ The further apart the points are, the lower the density\n➔ Local Outlier Factor (LOF)\n    ➔ Ratio of the average LRD of the K neighbors of A to the LRD of A.\n    ➔ Outliers are generally considered points with an LOF > 1, but this is also dependent on the LOFs of the neighboring points\n[[h3]]Autoencoders[[/]]\n➔ Description\n    ➔ Neural Network that learns encodings, or representations, for specific datasets\n    ➔ Converts data into usable form by disregarding noise\n    ➔ We’ve actually used Autoencoders before in the form of Sklearn’s Labelencoder module\n[[h3]]Applications of Unsupervised Learning[[/]]\n➔ Drug Discovery\n➔ Social Media Popularity Prediction\n➔ Machine Translation\n➔ Document Classification"
                }
            ]
        }
    ]
}